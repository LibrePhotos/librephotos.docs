"use strict";(self.webpackChunklibrephotos_docs=self.webpackChunklibrephotos_docs||[]).push([[8171],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>h});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=o.createContext({}),d=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=d(e.components);return o.createElement(s.Provider,{value:t},e.children)},u="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},g=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=d(n),g=a,h=u["".concat(s,".").concat(g)]||u[g]||p[g]||i;return n?o.createElement(h,r(r({ref:t},c),{},{components:n})):o.createElement(h,r({ref:t},c))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=g;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:a,r[1]=l;for(var d=2;d<i;d++)r[d]=n[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}g.displayName="MDXCreateElement"},2249:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var o=n(7462),a=(n(7294),n(3905));const i={title:"\u2601\ufe0f Backend",excerpt:"Development Information regarding LibrePhotos Backend.",last_modified_at:new Date("2020-08-04T00:00:00.000Z"),category:5},r=void 0,l={unversionedId:"development/contribution/backend/index",id:"development/contribution/backend/index",title:"\u2601\ufe0f Backend",description:"The backend uses the following technologies:",source:"@site/docs/development/contribution/backend/index.md",sourceDirName:"development/contribution/backend",slug:"/development/contribution/backend/",permalink:"/docs/development/contribution/backend/",draft:!1,editUrl:"https://github.com/LibrePhotos/librephotos.docs/tree/master/docs/development/contribution/backend/index.md",tags:[],version:"current",frontMatter:{title:"\u2601\ufe0f Backend",excerpt:"Development Information regarding LibrePhotos Backend.",last_modified_at:"2020-08-04T00:00:00.000Z",category:5},sidebar:"userguide",previous:{title:"Contribution",permalink:"/docs/development/contribution/"},next:{title:" \ud83d\uddbc Photo List",permalink:"/docs/development/contribution/backend/photo-list"}},s={},d=[{value:"\u2728 Code Standards",id:"-code-standards",level:2},{value:"\ud83d\udc1b Debugging",id:"-debugging",level:2},{value:"Using pdb",id:"using-pdb",level:3},{value:"Using silk",id:"using-silk",level:3},{value:"\ud83c\udfd9\ufe0f Structure",id:"\ufe0f-structure",level:2},{value:"Django",id:"django",level:3},{value:"management",id:"management",level:4},{value:"migrations",id:"migrations",level:4},{value:"models",id:"models",level:4},{value:"views",id:"views",level:4},{value:"serializer",id:"serializer",level:4},{value:"Machine Learning",id:"machine-learning",level:3},{value:"im2txt",id:"im2txt",level:4},{value:"Face Recognition",id:"face-recognition",level:4},{value:"Tagging Models",id:"tagging-models",level:4},{value:"Semantic Search",id:"semantic-search",level:4}],c={toc:d},u="wrapper";function p(e){let{components:t,...n}=e;return(0,a.kt)(u,(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("p",null,"The backend uses the following technologies:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Python"),(0,a.kt)("li",{parentName:"ul"},"Django"),(0,a.kt)("li",{parentName:"ul"},"Pytorch")),(0,a.kt)("h2",{id:"-code-standards"},"\u2728 Code Standards"),(0,a.kt)("p",null,"In order to have a similar structure to the code, the backend repository has a pre-commit hook. Just use pre-commit install in the folder and then the linter and the formatter will check it before you commit your actual work."),(0,a.kt)("p",null,"We use black, flake8 and isort to keep our code tidy."),(0,a.kt)("h2",{id:"-debugging"},"\ud83d\udc1b Debugging"),(0,a.kt)("p",null,"Usually nothing goes as planned, and you need to debug your shiny new feature. In the next paragraph, I will explain to you the two tools we have for debugging:"),(0,a.kt)("h3",{id:"using-pdb"},"Using pdb"),(0,a.kt)("p",null,"Add the following in the python code where you want a breakpoint:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"import pdb; pdb.set_trace()\n")),(0,a.kt)("p",null,"Attach to the backend service:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"docker attach $(docker ps --filter name=backend -q)\n")),(0,a.kt)("p",null,"Debug as normal in pdb!"),(0,a.kt)("p",null,"When you're done debugging, continue execution (c) and press Ctrl-P followed by Ctrl-Q to detach from the container without stopping it."),(0,a.kt)("h3",{id:"using-silk"},"Using silk"),(0,a.kt)("p",null,"In order to debug queries, start the backend container in dev mode. Then you can access silk under /api/silk. Silk is a live profiling and inspection tool for the Django framework. Silk intercepts and stores HTTP requests and database queries before presenting them in a user interface for further inspection."),(0,a.kt)("h2",{id:"\ufe0f-structure"},"\ud83c\udfd9\ufe0f Structure"),(0,a.kt)("p",null,"There are a lot of folders in our backend. Here is a quick rundown on where you can find what."),(0,a.kt)("h3",{id:"django"},"Django"),(0,a.kt)("p",null,"Most of the application code is within the API folder using Django. In the following section, I will explain where you can find what"),(0,a.kt)("h4",{id:"management"},"management"),(0,a.kt)("p",null,"This exposes all the commands that you can use via the command line. If you want a new command, that's the place to add it."),(0,a.kt)("h4",{id:"migrations"},"migrations"),(0,a.kt)("p",null,"Every time we change our models, we have to migrate the database. We use Django migration feature to create migration files to migrate without the headaches."),(0,a.kt)("h4",{id:"models"},"models"),(0,a.kt)("p",null,"Here are the actual data types. If you want to figure out how a photo works or how faces are connected to persons, then this is your folder."),(0,a.kt)("h4",{id:"views"},"views"),(0,a.kt)("p",null,"Here you can find our API implemented. They are separated, similar to the models. Views that expose the photos will be here in photos too."),(0,a.kt)("h4",{id:"serializer"},"serializer"),(0,a.kt)("p",null,"You have your python model and want to somehow convert that to JSON. That's what the serializer does!"),(0,a.kt)("h3",{id:"machine-learning"},"Machine Learning"),(0,a.kt)("p",null,"We use as a base framework PyTorch. If you find a cool machine learning model with PyTorch, we sure can add that too."),(0,a.kt)("h4",{id:"im2txt"},"im2txt"),(0,a.kt)("p",null,"im2txt is an image captioning package which allows us to generate captions on demand. This sometimes creates useful output, but it is kind of old and there should be more recent models"),(0,a.kt)("h4",{id:"face-recognition"},"Face Recognition"),(0,a.kt)("p",null,"We use dlib and face_recognition to detect faces. A very cool feature would be the automatic clustering of unknown faces, which we have not implemented yet."),(0,a.kt)("h4",{id:"tagging-models"},"Tagging Models"),(0,a.kt)("p",null,"The tags service (",(0,a.kt)("inlineCode",{parentName:"p"},"service/tags/"),") generates auto-tags for photos. Three models are available, selectable via the Tagging Model site setting:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"places365"),' \u2014 Scene classification using the Places365 CNN. Generates scene category tags (e.g. "kitchen", "beach").'),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"joytag")," (",(0,a.kt)("inlineCode",{parentName:"li"},"service/tags/joytag/"),") \u2014 ONNX-based content tagger with 5000+ tags. Returns the top 15 tags sorted by sigmoid confidence."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"siglip2")," (",(0,a.kt)("inlineCode",{parentName:"li"},"service/tags/siglip2/"),") \u2014 Google's SigLIP 2 vision-language model running as ONNX. Uses zero-shot classification by computing cosine similarity between image embeddings and a curated vocabulary of 900+ text tag embeddings. Tag embeddings are cached to disk after the first run. Returns the top 10 tags.")),(0,a.kt)("p",null,"Tags from each model are stored independently in ",(0,a.kt)("inlineCode",{parentName:"p"},"PhotoCaption.captions_json")," under their model key (e.g. ",(0,a.kt)("inlineCode",{parentName:"p"},'"places365"'),", ",(0,a.kt)("inlineCode",{parentName:"p"},'"joytag"'),", ",(0,a.kt)("inlineCode",{parentName:"p"},'"siglip2"'),"), so switching models does not require regeneration."),(0,a.kt)("h4",{id:"semantic-search"},"Semantic Search"),(0,a.kt)("p",null,'Here you can find the code which allows us to search semantically for images like "trees in a valley".'))}p.isMDXComponent=!0}}]);